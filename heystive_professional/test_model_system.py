#!/usr/bin/env python3
"""
Test Persian TTS Model System - Simple Version
ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ - Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡
"""

import os
import sys
import platform
from pathlib import Path

class SimpleHardwareDetector:
    """ØªØ´Ø®ÛŒØµ Ø³Ø§Ø¯Ù‡ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø®Ø§Ø±Ø¬ÛŒ"""
    
    def __init__(self):
        print("ğŸ” Simple Hardware Detection for Persian TTS")
        print("ØªØ´Ø®ÛŒØµ Ø³Ø§Ø¯Ù‡ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ø¨Ø±Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ")
        print("=" * 50)
        
        self.system_info = self._detect_basic_info()
        self.capability_level = self._determine_capability()
        self.recommended_models = self._get_recommendations()
    
    def _detect_basic_info(self):
        """ØªØ´Ø®ÛŒØµ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø³ÛŒØ³ØªÙ…"""
        info = {
            'platform': platform.system(),
            'architecture': platform.architecture()[0],
            'processor': platform.processor() or 'Unknown',
            'python_version': platform.python_version()
        }
        
        # ØªØ´Ø®ÛŒØµ ØªÙ‚Ø±ÛŒØ¨ÛŒ RAM (Ø¨Ø¯ÙˆÙ† psutil)
        try:
            if info['platform'] == 'Linux':
                with open('/proc/meminfo', 'r') as f:
                    meminfo = f.read()
                    for line in meminfo.split('\n'):
                        if 'MemTotal' in line:
                            ram_kb = int(line.split()[1])
                            info['ram_gb'] = round(ram_kb / (1024 * 1024), 1)
                            break
            else:
                # ØªØ®Ù…ÛŒÙ† Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØ± Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§
                info['ram_gb'] = 8.0  # ØªØ®Ù…ÛŒÙ† Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        except:
            info['ram_gb'] = 4.0  # ØªØ®Ù…ÛŒÙ† Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡
        
        # ØªØ´Ø®ÛŒØµ GPU (Ø³Ø§Ø¯Ù‡)
        info['has_gpu'] = self._detect_gpu()
        
        print(f"ğŸ’» Platform: {info['platform']} {info['architecture']}")
        print(f"ğŸ Python: {info['python_version']}")
        print(f"ğŸ’¾ Estimated RAM: {info['ram_gb']}GB")
        print(f"ğŸ® GPU Detected: {'Yes' if info['has_gpu'] else 'No'}")
        
        return info
    
    def _detect_gpu(self):
        """ØªØ´Ø®ÛŒØµ Ø³Ø§Ø¯Ù‡ GPU"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ CUDA
            import subprocess
            result = subprocess.run(['nvidia-smi'], 
                                  capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except:
            return False
    
    def _determine_capability(self):
        """ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ù‚Ø§Ø¨Ù„ÛŒØª Ø³ÛŒØ³ØªÙ…"""
        ram_gb = self.system_info.get('ram_gb', 0)
        has_gpu = self.system_info.get('has_gpu', False)
        
        if has_gpu and ram_gb >= 8:
            level = "HIGH_END"
            print("ğŸš€ High-end system capabilities detected")
        elif has_gpu and ram_gb >= 4:
            level = "MEDIUM"
            print("âš¡ Medium system capabilities detected")
        elif ram_gb >= 4:
            level = "CPU_OPTIMIZED"
            print("ğŸ’» CPU-optimized system detected")
        else:
            level = "LOW_END"
            print("ğŸ”‹ Low-resource system detected")
        
        return level
    
    def _get_recommendations(self):
        """Ø¯Ø±ÛŒØ§ÙØª ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„"""
        recommendations = {
            "HIGH_END": [
                {
                    "name": "XTTS-v2",
                    "quality": "Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ",
                    "size_gb": 2.0,
                    "huggingface_id": "coqui/XTTS-v2",
                    "features": ["Voice Cloning", "Studio Quality"],
                    "requirements": "8GB+ VRAM"
                },
                {
                    "name": "ParsiTTS",
                    "quality": "Ø¨Ø§Ù„Ø§",
                    "size_gb": 0.8,
                    "huggingface_id": "persiannlp/parsi-tts",
                    "features": ["High Quality", "Persian Optimized"],
                    "requirements": "4GB+ RAM, GPU recommended"
                }
            ],
            
            "MEDIUM": [
                {
                    "name": "ParsiTTS",
                    "quality": "Ø¨Ø§Ù„Ø§",
                    "size_gb": 0.8,
                    "huggingface_id": "persiannlp/parsi-tts",
                    "features": ["High Quality", "Persian Optimized"],
                    "requirements": "4GB+ RAM, GPU recommended"
                },
                {
                    "name": "VITS-Persian",
                    "quality": "Ø¨Ø§Ù„Ø§",
                    "size_gb": 0.6,
                    "huggingface_id": "persian-tts/vits-persian",
                    "features": ["VITS Architecture", "Persian Tokenization"],
                    "requirements": "4GB+ RAM"
                }
            ],
            
            "CPU_OPTIMIZED": [
                {
                    "name": "Silta Persian TTS",
                    "quality": "Ù…ØªÙˆØ³Ø·",
                    "size_gb": 0.2,
                    "huggingface_id": "persian-tts/silta-persian",
                    "features": ["Lightweight", "CPU Optimized"],
                    "requirements": "2GB+ RAM, CPU only"
                }
            ],
            
            "LOW_END": [
                {
                    "name": "Basic Persian TTS",
                    "quality": "Ù¾Ø§ÛŒÙ‡",
                    "size_gb": 0.1,
                    "huggingface_id": "persian-tts/basic-tts",
                    "features": ["Very Lightweight"],
                    "requirements": "1GB+ RAM"
                }
            ]
        }
        
        return recommendations.get(self.capability_level, [])
    
    def get_optimal_model(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„"""
        return self.recommended_models[0] if self.recommended_models else None
    
    def print_recommendations(self):
        """Ù†Ù…Ø§ÛŒØ´ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§"""
        print(f"\nğŸ’¡ Recommended Models for {self.capability_level}:")
        print("-" * 50)
        
        for i, model in enumerate(self.recommended_models, 1):
            print(f"{i}. {model['name']} ({model['quality']})")
            print(f"   ğŸ“¦ Size: {model['size_gb']}GB")
            print(f"   ğŸ”— Hugging Face: {model['huggingface_id']}")
            print(f"   âš™ï¸ Requirements: {model['requirements']}")
            print(f"   âœ¨ Features: {', '.join(model['features'])}")
            print()

class SimpleModelManager:
    """Ù…Ø¯ÛŒØ± Ø³Ø§Ø¯Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
    
    def __init__(self):
        self.models_dir = Path("heystive/models/persian_tts")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        self.hardware_detector = SimpleHardwareDetector()
        
        print("\nğŸ§  Simple Model Manager Initialized")
        print("Ù…Ø¯ÛŒØ± Ø³Ø§Ø¯Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
        print("=" * 50)
    
    def simulate_model_download(self, model_info):
        """Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„"""
        model_name = model_info['name']
        model_dir = self.models_dir / model_name.lower().replace(' ', '_')
        model_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“¥ Simulating download of: {model_name}")
        print(f"ğŸ”— From: {model_info['huggingface_id']}")
        print(f"ğŸ“¦ Size: {model_info['size_gb']}GB")
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
        config_file = model_dir / "config.json"
        model_file = model_dir / "model_info.txt"
        
        # Ø§ÛŒØ¬Ø§Ø¯ config Ù†Ù…ÙˆÙ†Ù‡
        import json
        config = {
            "model_name": model_name,
            "quality": model_info['quality'],
            "size_gb": model_info['size_gb'],
            "huggingface_id": model_info['huggingface_id'],
            "features": model_info['features'],
            "requirements": model_info['requirements'],
            "downloaded": True,
            "download_simulation": True
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        with open(model_file, 'w', encoding='utf-8') as f:
            f.write(f"Persian TTS Model: {model_name}\n")
            f.write(f"Quality: {model_info['quality']}\n")
            f.write(f"Size: {model_info['size_gb']}GB\n")
            f.write(f"Features: {', '.join(model_info['features'])}\n")
            f.write(f"Requirements: {model_info['requirements']}\n")
            f.write(f"Hugging Face: {model_info['huggingface_id']}\n")
            f.write("\nThis is a simulation file for testing purposes.\n")
            f.write("In real implementation, this would contain the actual model files.\n")
        
        print(f"âœ… Simulation complete: {model_dir}")
        return str(model_dir)
    
    def setup_optimal_model(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡"""
        optimal_model = self.hardware_detector.get_optimal_model()
        
        if optimal_model:
            print(f"\nğŸ¯ Setting up optimal model: {optimal_model['name']}")
            model_path = self.simulate_model_download(optimal_model)
            
            print(f"âœ… Optimal model ready at: {model_path}")
            return optimal_model
        else:
            print("âŒ No optimal model found")
            return None
    
    def generate_sample_tts(self, text: str = "Ø¨Ù„Ù‡ Ø³Ø±ÙˆØ±Ù…"):
        """ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡ TTS"""
        optimal_model = self.hardware_detector.get_optimal_model()
        
        if not optimal_model:
            print("âŒ No model available for TTS")
            return None
        
        print(f"\nğŸ¤ Generating TTS Sample")
        print(f"ğŸ“ Text: {text}")
        print(f"ğŸ¤– Model: {optimal_model['name']} ({optimal_model['quality']})")
        
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ØªÙˆÙ„ÛŒØ¯ ØµÙˆØª
        output_file = f"tts_sample_{optimal_model['name'].lower().replace(' ', '_')}.wav"
        output_path = Path("audio_output") / output_file
        output_path.parent.mkdir(exist_ok=True)
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ù†Ù…ÙˆÙ†Ù‡
        sample_content = f"""# Persian TTS Sample - {optimal_model['name']}
Text: {text}
Model: {optimal_model['name']}
Quality: {optimal_model['quality']}
Generated with: Heystive Persian TTS System

This is a simulation file. In real implementation, 
this would be an actual audio file (.wav) containing 
the synthesized Persian speech.

Model Information:
- Size: {optimal_model['size_gb']}GB
- Features: {', '.join(optimal_model['features'])}
- Requirements: {optimal_model['requirements']}
- Hugging Face: {optimal_model['huggingface_id']}
"""
        
        with open(str(output_path).replace('.wav', '.txt'), 'w', encoding='utf-8') as f:
            f.write(sample_content)
        
        print(f"âœ… TTS sample generated: {output_path}")
        print(f"ğŸ“ Location: {output_path.parent}")
        
        return str(output_path)

def main():
    """ØªØ³Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
    print("ğŸš€ Testing Persian TTS Model System")
    print("ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ")
    print("=" * 60)
    
    try:
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯ÛŒØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§
        manager = SimpleModelManager()
        
        # Ù†Ù…Ø§ÛŒØ´ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
        manager.hardware_detector.print_recommendations()
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡
        optimal_model = manager.setup_optimal_model()
        
        if optimal_model:
            # ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡ ØµÙˆØªÛŒ
            sample_path = manager.generate_sample_tts("Ø¨Ù„Ù‡ Ø³Ø±ÙˆØ±Ù…")
            
            print(f"\nğŸ‰ SUCCESS! Persian TTS System Ready")
            print(f"âœ… Optimal Model: {optimal_model['name']}")
            print(f"âœ… Sample Generated: {sample_path}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡
        print(f"\nğŸ“Š SYSTEM SUMMARY")
        print("=" * 30)
        print(f"Hardware Level: {manager.hardware_detector.capability_level}")
        print(f"Platform: {manager.hardware_detector.system_info['platform']}")
        print(f"RAM: {manager.hardware_detector.system_info['ram_gb']}GB")
        print(f"GPU: {'Available' if manager.hardware_detector.system_info['has_gpu'] else 'Not Available'}")
        
        if optimal_model:
            print(f"Selected Model: {optimal_model['name']} ({optimal_model['quality']})")
            print(f"Model Size: {optimal_model['size_gb']}GB")
        
        print(f"\nğŸ¯ Model System Test: SUCCESSFUL âœ…")
        return True
        
    except Exception as e:
        print(f"âŒ Model system test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()