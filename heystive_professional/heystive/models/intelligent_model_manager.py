#!/usr/bin/env python3
"""
Intelligent Persian TTS Model Manager
Ù…Ø¯ÛŒØ± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
import logging

from .hardware_detector import HardwareDetector
from .model_downloader import PersianTTSModelDownloader

logger = logging.getLogger(__name__)

class IntelligentModelManager:
    """
    Ù…Ø¯ÛŒØ± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ
    - ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±
    - Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ÛŒÙ†Ù‡ Ù…Ø¯Ù„
    - Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§
    - Ø³ÛŒØ³ØªÙ… fallback
    """
    
    def __init__(self, models_dir: str = None):
        self.models_dir = Path(models_dir) if models_dir else Path(__file__).parent / "persian_tts"
        
        print("ğŸ§  Intelligent Persian TTS Model Manager")
        print("Ù…Ø¯ÛŒØ± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ")
        print("=" * 60)
        
        # Ø§Ø¬Ø²Ø§ÛŒ Ø³ÛŒØ³ØªÙ…
        self.hardware_detector = HardwareDetector()
        self.model_downloader = PersianTTSModelDownloader(str(self.models_dir))
        
        # ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…
        self.system_capability = self.hardware_detector.system_info.get('capability_level', 'LOW_END')
        self.active_model = None
        self.fallback_models = []
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        self.auto_download = True
        self.max_models_to_keep = 3
        
        print(f"ğŸ¯ System Capability: {self.system_capability}")
        
        self._initialize_system()
    
    def _initialize_system(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø³ÛŒØ³ØªÙ…"""
        print("\nğŸ”§ Initializing Model Management System...")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        downloaded_models = self.model_downloader.get_downloaded_models()
        
        if downloaded_models:
            print(f"âœ… Found {len(downloaded_models)} downloaded models")
            for model in downloaded_models:
                print(f"   ğŸ“¦ {model['name']} ({model.get('quality', 'N/A')})")
        else:
            print("ğŸ“¥ No models downloaded yet")
        
        # Ø§Ù†ØªØ®Ø§Ø¨ ÛŒØ§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡
        self._setup_optimal_model()
        
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… fallback
        self._setup_fallback_system()
    
    def _setup_optimal_model(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡"""
        print("\nğŸ¯ Setting up optimal model...")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¯Ù„ ØªÙˆØµÛŒÙ‡ Ø´Ø¯Ù‡
        optimal_model = self.hardware_detector.get_optimal_model()
        
        if not optimal_model:
            print("âŒ No optimal model found for current hardware")
            return
        
        print(f"ğŸ’¡ Recommended: {optimal_model['name']} ({optimal_model['quality']})")
        
        # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ø¨Ù‡ ID
        model_id = self._get_model_id_by_name(optimal_model['name'])
        
        if not model_id:
            print(f"âŒ Model ID not found for: {optimal_model['name']}")
            return
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†
        if self.model_downloader.is_model_downloaded(model_id):
            print(f"âœ… Optimal model already downloaded: {optimal_model['name']}")
            self.active_model = model_id
        else:
            if self.auto_download:
                print(f"ğŸ“¥ Auto-downloading optimal model...")
                
                # Ø¨Ø±Ø±Ø³ÛŒ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú©
                if not self.hardware_detector.can_download_model(optimal_model['size_gb']):
                    print(f"âš ï¸ Insufficient disk space for {optimal_model['name']}")
                    self._try_alternative_models()
                    return
                
                # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„
                if self.model_downloader.download_model(model_id):
                    self.active_model = model_id
                    print(f"ğŸ‰ Successfully set up optimal model: {optimal_model['name']}")
                else:
                    print(f"âŒ Failed to download optimal model")
                    self._try_alternative_models()
            else:
                print(f"âš ï¸ Auto-download disabled. Please download manually: {optimal_model['name']}")
    
    def _try_alternative_models(self):
        """ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†"""
        print("\nğŸ”„ Trying alternative models...")
        
        compatible_models = self.hardware_detector.get_all_compatible_models()
        
        for model_info in compatible_models[1:]:  # Ø§Ø² Ù…Ø¯Ù„ Ø¯ÙˆÙ… Ø´Ø±ÙˆØ¹ Ú©Ù† (Ø§ÙˆÙ„ optimal Ø¨ÙˆØ¯)
            model_id = self._get_model_id_by_name(model_info['name'])
            
            if not model_id:
                continue
            
            if self.model_downloader.is_model_downloaded(model_id):
                print(f"âœ… Using alternative model: {model_info['name']}")
                self.active_model = model_id
                return
            
            if self.auto_download and self.hardware_detector.can_download_model(model_info['size_gb']):
                print(f"ğŸ“¥ Downloading alternative: {model_info['name']}")
                if self.model_downloader.download_model(model_id):
                    self.active_model = model_id
                    print(f"âœ… Successfully set up alternative: {model_info['name']}")
                    return
        
        print("âŒ No suitable models could be set up")
    
    def _setup_fallback_system(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… fallback"""
        print("\nğŸ›¡ï¸ Setting up fallback system...")
        
        downloaded_models = self.model_downloader.get_downloaded_models()
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©ÛŒÙÛŒØª Ùˆ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ
        quality_order = {"Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ": 4, "Ø¨Ø§Ù„Ø§": 3, "Ù…ØªÙˆØ³Ø·": 2, "Ù¾Ø§ÛŒÙ‡": 1}
        
        sorted_models = sorted(downloaded_models, 
                             key=lambda x: quality_order.get(x.get('quality', 'Ù¾Ø§ÛŒÙ‡'), 0), 
                             reverse=True)
        
        self.fallback_models = [model['id'] for model in sorted_models 
                               if model['id'] != self.active_model]
        
        if self.fallback_models:
            print(f"âœ… Fallback models available: {len(self.fallback_models)}")
            for model_id in self.fallback_models[:3]:  # Ù†Ù…Ø§ÛŒØ´ 3 Ù…Ø¯Ù„ Ø§ÙˆÙ„
                model_info = self._get_model_info(model_id)
                if model_info:
                    print(f"   ğŸ”„ {model_info.get('name', model_id)}")
        else:
            print("âš ï¸ No fallback models available")
    
    def _get_model_id_by_name(self, model_name: str) -> Optional[str]:
        """Ø¯Ø±ÛŒØ§ÙØª ID Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù…"""
        for model_id, config in self.model_downloader.model_configs.items():
            if config["name"] == model_name:
                return model_id
        return None
    
    def _get_model_info(self, model_id: str) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„"""
        model_path = self.models_dir / model_id / "model_info.json"
        
        if model_path.exists():
            try:
                with open(model_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load model info for {model_id}: {e}")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø§Ø² config
        if model_id in self.model_downloader.model_configs:
            config = self.model_downloader.model_configs[model_id]
            return {
                "id": model_id,
                "name": config["name"],
                "quality": config["quality"]
            }
        
        return None
    
    def get_active_model(self) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¯Ù„ ÙØ¹Ø§Ù„"""
        if self.active_model:
            return self._get_model_info(self.active_model)
        return None
    
    def switch_model(self, model_id: str) -> bool:
        """ØªØºÛŒÛŒØ± Ù…Ø¯Ù„ ÙØ¹Ø§Ù„"""
        if not self.model_downloader.is_model_downloaded(model_id):
            print(f"âŒ Model '{model_id}' is not downloaded")
            return False
        
        old_model = self.active_model
        self.active_model = model_id
        
        print(f"ğŸ”„ Switched from {old_model} to {model_id}")
        return True
    
    def download_recommended_models(self, max_models: int = 2) -> List[str]:
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙ‡ Ø´Ø¯Ù‡"""
        print(f"\nğŸ“¥ Downloading up to {max_models} recommended models...")
        
        compatible_models = self.hardware_detector.get_all_compatible_models()
        downloaded_models = []
        
        for i, model_info in enumerate(compatible_models[:max_models]):
            model_id = self._get_model_id_by_name(model_info['name'])
            
            if not model_id:
                continue
            
            if self.model_downloader.is_model_downloaded(model_id):
                print(f"â­ï¸ Already downloaded: {model_info['name']}")
                downloaded_models.append(model_id)
                continue
            
            if not self.hardware_detector.can_download_model(model_info['size_gb']):
                print(f"âš ï¸ Insufficient space for: {model_info['name']}")
                continue
            
            print(f"\nğŸ“¦ Downloading {i+1}/{max_models}: {model_info['name']}")
            
            if self.model_downloader.download_model(model_id):
                downloaded_models.append(model_id)
                print(f"âœ… Downloaded: {model_info['name']}")
            else:
                print(f"âŒ Failed to download: {model_info['name']}")
        
        return downloaded_models
    
    def optimize_storage(self):
        """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ¶Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        print("\nğŸ—‚ï¸ Optimizing storage...")
        
        downloaded_models = self.model_downloader.get_downloaded_models()
        
        if len(downloaded_models) <= self.max_models_to_keep:
            print("âœ… Storage already optimized")
            return
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÙˆÙ„ÙˆÛŒØª (Ú©ÛŒÙÛŒØª Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø®ÛŒØ±)
        quality_order = {"Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ": 4, "Ø¨Ø§Ù„Ø§": 3, "Ù…ØªÙˆØ³Ø·": 2, "Ù¾Ø§ÛŒÙ‡": 1}
        
        sorted_models = sorted(downloaded_models,
                             key=lambda x: (
                                 x['id'] == self.active_model,  # Ù…Ø¯Ù„ ÙØ¹Ø§Ù„ Ø§ÙˆÙ„ÙˆÛŒØª Ø¯Ø§Ø±Ø¯
                                 quality_order.get(x.get('quality', 'Ù¾Ø§ÛŒÙ‡'), 0)
                             ),
                             reverse=True)
        
        # Ø­Ø°Ù Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        models_to_remove = sorted_models[self.max_models_to_keep:]
        
        for model in models_to_remove:
            if model['id'] != self.active_model:  # Ù…Ø¯Ù„ ÙØ¹Ø§Ù„ Ø±Ø§ Ø­Ø°Ù Ù†Ú©Ù†
                print(f"ğŸ—‘ï¸ Removing: {model['name']}")
                self.model_downloader.remove_model(model['id'])
    
    def get_system_status(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª ÙˆØ¶Ø¹ÛŒØª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ…"""
        storage = self.model_downloader.get_storage_usage()
        active_model = self.get_active_model()
        
        return {
            "hardware": {
                "capability_level": self.system_capability,
                "ram_gb": self.hardware_detector.system_info.get('ram_available_gb', 0),
                "gpu_available": self.hardware_detector.gpu_info.get('has_gpu', False),
                "gpu_memory_gb": self.hardware_detector.gpu_info.get('gpu_memory_gb', 0)
            },
            "models": {
                "active_model": active_model,
                "downloaded_count": storage['models_count'],
                "total_size_mb": storage['total_mb'],
                "fallback_models": len(self.fallback_models)
            },
            "recommendations": self.hardware_detector.get_all_compatible_models()
        }
    
    def generate_tts_audio(self, text: str, output_path: str = None) -> Optional[str]:
        """ØªÙˆÙ„ÛŒØ¯ ØµÙˆØª TTS (Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª)"""
        if not self.active_model:
            print("âŒ No active model for TTS generation")
            return None
        
        print(f"ğŸ¤ Generating TTS with model: {self.active_model}")
        print(f"ğŸ“ Text: {text}")
        
        # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ Ú©Ø¯ ÙˆØ§Ù‚Ø¹ÛŒ TTS Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ø¯
        # ÙØ¹Ù„Ø§Ù‹ ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
        
        if not output_path:
            output_path = f"tts_output_{int(time.time())}.wav"
        
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ØªÙˆÙ„ÛŒØ¯ ØµÙˆØª
        print(f"ğŸ”„ Processing text with {self.active_model}...")
        time.sleep(1)  # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
        
        # Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ù†Ù…ÙˆÙ†Ù‡ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª)
        try:
            with open(output_path, 'wb') as f:
                f.write(b"FAKE_AUDIO_DATA_FOR_TESTING")
            
            print(f"âœ… TTS audio generated: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"TTS generation failed: {e}")
            return None

def main():
    """ØªØ³Øª Ù…Ø¯ÛŒØ± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
    print("ğŸ§  Testing Intelligent Model Manager")
    print("=" * 50)
    
    try:
        manager = IntelligentModelManager()
        
        print("\n" + "=" * 50)
        print("ğŸ“Š SYSTEM STATUS REPORT")
        print("Ú¯Ø²Ø§Ø±Ø´ ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒØ³ØªÙ…")
        print("=" * 50)
        
        status = manager.get_system_status()
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±
        hw = status['hardware']
        print(f"ğŸ–¥ï¸ Hardware Capability: {hw['capability_level']}")
        print(f"ğŸ’¾ Available RAM: {hw['ram_gb']:.1f}GB")
        print(f"ğŸ® GPU: {'Yes' if hw['gpu_available'] else 'No'}")
        if hw['gpu_available']:
            print(f"   VRAM: {hw['gpu_memory_gb']:.1f}GB")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§
        models = status['models']
        print(f"\nğŸ“¦ Models Status:")
        print(f"   Downloaded: {models['downloaded_count']} models")
        print(f"   Storage Used: {models['total_size_mb']:.1f}MB")
        print(f"   Fallback Options: {models['fallback_models']}")
        
        active_model = models['active_model']
        if active_model:
            print(f"   Active Model: {active_model['name']} ({active_model.get('quality', 'N/A')})")
        else:
            print("   Active Model: None")
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…
        print(f"\nğŸ’¡ Recommendations:")
        for i, rec in enumerate(status['recommendations'][:3], 1):
            print(f"   {i}. {rec['name']} ({rec['quality']})")
            print(f"      Size: {rec['size_gb']}GB | {rec['requirements']}")
        
        # ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ ØµÙˆØª
        print(f"\nğŸ¤ Testing TTS Generation:")
        result = manager.generate_tts_audio("Ø¨Ù„Ù‡ Ø³Ø±ÙˆØ±Ù…", "test_tts_output.wav")
        
        if result:
            print(f"âœ… TTS Test Successful: {result}")
        else:
            print("âŒ TTS Test Failed")
        
        return manager
        
    except Exception as e:
        logger.error(f"Manager test failed: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()