#!/usr/bin/env python3
"""
Persian TTS Model Downloader with Intelligent Management
Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ Ø§Ø² Hugging Face
"""

import os
import sys
import json
import time
import hashlib
import requests
from pathlib import Path
from typing import Dict, List, Optional, Callable
from urllib.parse import urlparse
import logging

logger = logging.getLogger(__name__)

class PersianTTSModelDownloader:
    """
    Ø¯Ø§Ù†Ù„ÙˆØ¯Ú©Ù†Ù†Ø¯Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ
    """
    
    def __init__(self, models_dir: str = None):
        self.models_dir = Path(models_dir) if models_dir else Path(__file__).parent / "persian_tts"
        self.cache_dir = self.models_dir.parent / "cache"
        self.configs_dir = self.models_dir.parent / "configs"
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.configs_dir.mkdir(parents=True, exist_ok=True)
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Heystive-Persian-TTS/1.0'
        })
        
        print("ğŸ“¥ Persian TTS Model Downloader")
        print("Ø¯Ø§Ù†Ù„ÙˆØ¯Ú©Ù†Ù†Ø¯Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ")
        print("=" * 50)
        
        # Ù„ÙˆØ¯ Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        self._load_model_configs()
    
    def _load_model_configs(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        self.model_configs = {
            "parsi-tts": {
                "name": "ParsiTTS",
                "huggingface_id": "persiannlp/parsi-tts",
                "files": [
                    "config.json",
                    "pytorch_model.bin",
                    "tokenizer.json",
                    "vocab.txt"
                ],
                "size_mb": 800,
                "quality": "Ø¨Ø§Ù„Ø§",
                "requirements": ["torch", "transformers", "librosa"],
                "gpu_recommended": True
            },
            
            "vits-persian": {
                "name": "VITS-Persian", 
                "huggingface_id": "persian-tts/vits-persian",
                "files": [
                    "config.json",
                    "G_latest.pth",
                    "symbols.txt"
                ],
                "size_mb": 600,
                "quality": "Ø¨Ø§Ù„Ø§",
                "requirements": ["torch", "librosa", "phonemizer"],
                "gpu_recommended": True
            },
            
            "silta-persian": {
                "name": "Silta Persian TTS",
                "huggingface_id": "persian-tts/silta-persian", 
                "files": [
                    "config.json",
                    "model.onnx",
                    "tokenizer.json"
                ],
                "size_mb": 200,
                "quality": "Ù…ØªÙˆØ³Ø·",
                "requirements": ["onnxruntime", "librosa"],
                "gpu_recommended": False
            },
            
            "xtts-v2": {
                "name": "XTTS-v2",
                "huggingface_id": "coqui/XTTS-v2",
                "files": [
                    "config.json",
                    "model.pth",
                    "vocab.json",
                    "speakers_xtts.pth"
                ],
                "size_mb": 2000,
                "quality": "Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ",
                "requirements": ["torch", "torchaudio", "transformers"],
                "gpu_recommended": True,
                "gpu_required": True
            },
            
            "fastpitch-persian": {
                "name": "FastPitch-Persian",
                "huggingface_id": "persian-tts/fastpitch-persian",
                "files": [
                    "config.json", 
                    "fastpitch.pth",
                    "hifigan.pth"
                ],
                "size_mb": 400,
                "quality": "Ø¨Ø§Ù„Ø§",
                "requirements": ["torch", "librosa"],
                "gpu_recommended": True
            }
        }
    
    def list_available_models(self) -> List[Dict]:
        """Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³"""
        models = []
        for model_id, config in self.model_configs.items():
            model_info = {
                "id": model_id,
                "name": config["name"],
                "size_mb": config["size_mb"],
                "quality": config["quality"],
                "gpu_recommended": config.get("gpu_recommended", False),
                "gpu_required": config.get("gpu_required", False),
                "downloaded": self.is_model_downloaded(model_id)
            }
            models.append(model_info)
        
        return models
    
    def is_model_downloaded(self, model_id: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù† Ù…Ø¯Ù„"""
        if model_id not in self.model_configs:
            return False
        
        model_path = self.models_dir / model_id
        if not model_path.exists():
            return False
        
        config = self.model_configs[model_id]
        required_files = config.get("files", [])
        
        for file_name in required_files:
            file_path = model_path / file_name
            if not file_path.exists():
                return False
        
        return True
    
    def get_download_url(self, huggingface_id: str, filename: str) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ URL Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² Hugging Face"""
        return f"https://huggingface.co/{huggingface_id}/resolve/main/{filename}"
    
    def download_file(self, url: str, destination: Path, 
                     progress_callback: Optional[Callable] = None) -> bool:
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¨Ø§ progress bar"""
        try:
            print(f"ğŸ“¥ Downloading: {destination.name}")
            
            response = self.session.get(url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            with open(destination, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        if progress_callback and total_size > 0:
                            progress = (downloaded_size / total_size) * 100
                            progress_callback(progress, downloaded_size, total_size)
            
            print(f"âœ… Downloaded: {destination.name}")
            return True
            
        except Exception as e:
            logger.error(f"Download failed for {url}: {e}")
            if destination.exists():
                destination.unlink()  # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù†Ø§Ù‚Øµ
            return False
    
    def progress_callback(self, progress: float, downloaded: int, total: int):
        """Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª Ø¯Ø§Ù†Ù„ÙˆØ¯"""
        downloaded_mb = downloaded / (1024 * 1024)
        total_mb = total / (1024 * 1024)
        
        bar_length = 30
        filled_length = int(bar_length * progress / 100)
        bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)
        
        print(f"\r[{bar}] {progress:.1f}% ({downloaded_mb:.1f}/{total_mb:.1f} MB)", end='')
        
        if progress >= 100:
            print()  # Ø®Ø· Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ù¾Ø§ÛŒØ§Ù†
    
    def download_model(self, model_id: str, force_redownload: bool = False) -> bool:
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„"""
        if model_id not in self.model_configs:
            print(f"âŒ Model '{model_id}' not found in configs")
            return False
        
        if self.is_model_downloaded(model_id) and not force_redownload:
            print(f"âœ… Model '{model_id}' already downloaded")
            return True
        
        config = self.model_configs[model_id]
        model_path = self.models_dir / model_id
        model_path.mkdir(exist_ok=True)
        
        print(f"ğŸš€ Downloading model: {config['name']}")
        print(f"ğŸ“¦ Size: ~{config['size_mb']} MB")
        print(f"ğŸ·ï¸ Quality: {config['quality']}")
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
        huggingface_id = config["huggingface_id"]
        files_to_download = config.get("files", [])
        
        downloaded_files = 0
        total_files = len(files_to_download)
        
        for file_name in files_to_download:
            file_path = model_path / file_name
            
            if file_path.exists() and not force_redownload:
                print(f"â­ï¸ Skipping existing: {file_name}")
                downloaded_files += 1
                continue
            
            download_url = self.get_download_url(huggingface_id, file_name)
            
            if self.download_file(download_url, file_path, self.progress_callback):
                downloaded_files += 1
            else:
                print(f"âŒ Failed to download: {file_name}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„
        model_info_file = model_path / "model_info.json"
        model_info = {
            "id": model_id,
            "name": config["name"],
            "huggingface_id": huggingface_id,
            "download_date": time.strftime("%Y-%m-%d %H:%M:%S"),
            "files": files_to_download,
            "size_mb": config["size_mb"],
            "quality": config["quality"],
            "requirements": config.get("requirements", [])
        }
        
        with open(model_info_file, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, indent=2, ensure_ascii=False)
        
        success = downloaded_files == total_files
        
        if success:
            print(f"ğŸ‰ Model '{config['name']}' downloaded successfully!")
            print(f"ğŸ“ Location: {model_path}")
        else:
            print(f"âš ï¸ Partial download: {downloaded_files}/{total_files} files")
        
        return success
    
    def download_optimal_model(self, hardware_info: Dict) -> Optional[str]:
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±"""
        from .hardware_detector import HardwareDetector
        
        detector = HardwareDetector()
        optimal_model = detector.get_optimal_model()
        
        if not optimal_model:
            print("âŒ No optimal model found for this hardware")
            return None
        
        # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ù‡ ID
        model_id = None
        for mid, config in self.model_configs.items():
            if config["name"] == optimal_model["name"]:
                model_id = mid
                break
        
        if not model_id:
            print(f"âŒ Model ID not found for: {optimal_model['name']}")
            return None
        
        print(f"ğŸ¯ Downloading optimal model: {optimal_model['name']}")
        
        if self.download_model(model_id):
            return model_id
        else:
            print("âŒ Failed to download optimal model")
            return None
    
    def get_downloaded_models(self) -> List[Dict]:
        """Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡"""
        downloaded_models = []
        
        for model_id in self.model_configs.keys():
            if self.is_model_downloaded(model_id):
                model_path = self.models_dir / model_id
                info_file = model_path / "model_info.json"
                
                if info_file.exists():
                    with open(info_file, 'r', encoding='utf-8') as f:
                        model_info = json.load(f)
                    downloaded_models.append(model_info)
                else:
                    # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø§Ú¯Ø± ÙØ§ÛŒÙ„ info Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯
                    config = self.model_configs[model_id]
                    downloaded_models.append({
                        "id": model_id,
                        "name": config["name"],
                        "quality": config["quality"]
                    })
        
        return downloaded_models
    
    def remove_model(self, model_id: str) -> bool:
        """Ø­Ø°Ù Ù…Ø¯Ù„"""
        if not self.is_model_downloaded(model_id):
            print(f"âš ï¸ Model '{model_id}' is not downloaded")
            return False
        
        model_path = self.models_dir / model_id
        
        try:
            import shutil
            shutil.rmtree(model_path)
            print(f"ğŸ—‘ï¸ Model '{model_id}' removed successfully")
            return True
        except Exception as e:
            logger.error(f"Failed to remove model {model_id}: {e}")
            return False
    
    def get_storage_usage(self) -> Dict:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ¶Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        total_size = 0
        model_sizes = {}
        
        for model_id in self.model_configs.keys():
            if self.is_model_downloaded(model_id):
                model_path = self.models_dir / model_id
                size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file())
                model_sizes[model_id] = size
                total_size += size
        
        return {
            "total_mb": total_size / (1024 * 1024),
            "model_sizes_mb": {k: v / (1024 * 1024) for k, v in model_sizes.items()},
            "models_count": len(model_sizes)
        }

def main():
    """ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
    downloader = PersianTTSModelDownloader()
    
    print("\nğŸ“‹ Available Persian TTS Models:")
    print("-" * 40)
    
    models = downloader.list_available_models()
    for model in models:
        status = "âœ… Downloaded" if model["downloaded"] else "â¬‡ï¸ Available"
        gpu_req = "ğŸ® GPU" if model["gpu_required"] else "ğŸ’» CPU" if not model["gpu_recommended"] else "ğŸ® GPU Rec."
        
        print(f"{model['name']} ({model['quality']})")
        print(f"  {status} | {model['size_mb']}MB | {gpu_req}")
        print()
    
    # Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡
    downloaded = downloader.get_downloaded_models()
    if downloaded:
        print("ğŸ“¥ Downloaded Models:")
        for model in downloaded:
            print(f"  âœ… {model['name']} ({model.get('quality', 'N/A')})")
    
    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙØ¶Ø§
    storage = downloader.get_storage_usage()
    print(f"\nğŸ’¾ Storage Usage: {storage['total_mb']:.1f}MB ({storage['models_count']} models)")
    
    return downloader

if __name__ == "__main__":
    main()