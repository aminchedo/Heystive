#!/usr/bin/env python3
"""
Hardware Detection System for Optimal Persian TTS Model Selection
ØªØ´Ø®ÛŒØµ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ÛŒÙ†Ù‡ Ù…Ø¯Ù„ TTS ÙØ§Ø±Ø³ÛŒ
"""

import os
import sys
import psutil
import platform
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging

logger = logging.getLogger(__name__)

class HardwareDetector:
    """
    ØªØ´Ø®ÛŒØµ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ø³ÛŒØ³ØªÙ… Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ÛŒÙ†Ù‡ Ù…Ø¯Ù„ TTS
    """
    
    def __init__(self):
        self.system_info = {}
        self.gpu_info = {}
        self.recommended_models = []
        
        print("ğŸ” Hardware Detection for Persian TTS Models")
        print("ØªØ´Ø®ÛŒØµ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ TTS ÙØ§Ø±Ø³ÛŒ")
        print("=" * 50)
        
        self._detect_system_info()
        self._detect_gpu_info()
        self._analyze_capabilities()
    
    def _detect_system_info(self):
        """ØªØ´Ø®ÛŒØµ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø³ÛŒØ³ØªÙ…"""
        try:
            self.system_info = {
                'platform': platform.system(),
                'architecture': platform.architecture()[0],
                'processor': platform.processor() or 'Unknown',
                'cpu_count': psutil.cpu_count(logical=True),
                'cpu_count_physical': psutil.cpu_count(logical=False),
                'ram_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
                'ram_available_gb': round(psutil.virtual_memory().available / (1024**3), 2),
                'disk_free_gb': round(psutil.disk_usage('/').free / (1024**3), 2)
            }
            
            print(f"ğŸ’» System: {self.system_info['platform']} {self.system_info['architecture']}")
            print(f"ğŸ§  CPU: {self.system_info['cpu_count']} cores ({self.system_info['cpu_count_physical']} physical)")
            print(f"ğŸ’¾ RAM: {self.system_info['ram_available_gb']:.1f}GB available / {self.system_info['ram_total_gb']:.1f}GB total")
            print(f"ğŸ’½ Disk: {self.system_info['disk_free_gb']:.1f}GB free")
            
        except Exception as e:
            logger.error(f"System info detection failed: {e}")
    
    def _detect_gpu_info(self):
        """ØªØ´Ø®ÛŒØµ GPU Ùˆ CUDA"""
        self.gpu_info = {
            'has_gpu': False,
            'gpu_count': 0,
            'gpu_memory_gb': 0,
            'cuda_available': False,
            'gpu_names': []
        }
        
        # Check for NVIDIA GPU
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            
            if gpus:
                self.gpu_info['has_gpu'] = True
                self.gpu_info['gpu_count'] = len(gpus)
                self.gpu_info['gpu_memory_gb'] = max(gpu.memoryTotal / 1024 for gpu in gpus)
                self.gpu_info['gpu_names'] = [gpu.name for gpu in gpus]
                
                print(f"ğŸ® GPU: {self.gpu_info['gpu_count']} GPU(s) detected")
                for gpu in gpus:
                    print(f"   ğŸ“Š {gpu.name}: {gpu.memoryTotal/1024:.1f}GB VRAM")
            else:
                print("ğŸš« No NVIDIA GPU detected")
                
        except ImportError:
            print("âš ï¸ GPUtil not available - GPU detection limited")
        except Exception as e:
            logger.warning(f"GPU detection error: {e}")
        
        # Check CUDA availability
        try:
            import torch
            if torch.cuda.is_available():
                self.gpu_info['cuda_available'] = True
                cuda_version = torch.version.cuda
                print(f"âœ… CUDA available: {cuda_version}")
            else:
                print("âŒ CUDA not available")
        except ImportError:
            print("âš ï¸ PyTorch not installed - CUDA check skipped")
        except Exception as e:
            logger.warning(f"CUDA check error: {e}")
    
    def _analyze_capabilities(self):
        """ØªØ­Ù„ÛŒÙ„ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…"""
        ram_gb = self.system_info.get('ram_available_gb', 0)
        gpu_memory_gb = self.gpu_info.get('gpu_memory_gb', 0)
        has_gpu = self.gpu_info.get('has_gpu', False)
        cuda_available = self.gpu_info.get('cuda_available', False)
        
        print(f"\nğŸ¯ System Capability Analysis:")
        print("-" * 30)
        
        # ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±
        if has_gpu and cuda_available and gpu_memory_gb >= 8 and ram_gb >= 8:
            capability_level = "HIGH_END"
            print("ğŸš€ High-end system detected")
        elif has_gpu and cuda_available and gpu_memory_gb >= 4 and ram_gb >= 4:
            capability_level = "MEDIUM"
            print("âš¡ Medium-performance system detected")
        elif ram_gb >= 4:
            capability_level = "CPU_OPTIMIZED"
            print("ğŸ’» CPU-optimized system detected")
        else:
            capability_level = "LOW_END"
            print("ğŸ”‹ Low-resource system detected")
        
        self.system_info['capability_level'] = capability_level
        
        # ØªÙˆØµÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±
        self._recommend_models(capability_level)
    
    def _recommend_models(self, capability_level: str):
        """ØªÙˆØµÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±"""
        
        model_recommendations = {
            "HIGH_END": [
                {
                    "name": "XTTS-v2",
                    "huggingface_id": "coqui/XTTS-v2",
                    "priority": 1,
                    "features": ["Voice Cloning", "Studio Quality", "Multi-language"],
                    "requirements": "8GB+ VRAM, GPU required",
                    "size_gb": 2.0,
                    "quality": "Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ"
                },
                {
                    "name": "ParsiTTS",
                    "huggingface_id": "persiannlp/parsi-tts",
                    "priority": 2,
                    "features": ["High Quality", "VITS Architecture", "Persian Optimized"],
                    "requirements": "4GB+ RAM, GPU recommended",
                    "size_gb": 0.8,
                    "quality": "Ø¨Ø§Ù„Ø§"
                },
                {
                    "name": "VITS-Persian",
                    "huggingface_id": "persian-tts/vits-persian",
                    "priority": 3,
                    "features": ["VITS Architecture", "Persian Tokenization"],
                    "requirements": "4GB+ RAM, GPU recommended",
                    "size_gb": 0.6,
                    "quality": "Ø¨Ø§Ù„Ø§"
                }
            ],
            
            "MEDIUM": [
                {
                    "name": "ParsiTTS",
                    "huggingface_id": "persiannlp/parsi-tts",
                    "priority": 1,
                    "features": ["High Quality", "VITS Architecture", "Persian Optimized"],
                    "requirements": "4GB+ RAM, GPU recommended",
                    "size_gb": 0.8,
                    "quality": "Ø¨Ø§Ù„Ø§"
                },
                {
                    "name": "VITS-Persian",
                    "huggingface_id": "persian-tts/vits-persian",
                    "priority": 2,
                    "features": ["VITS Architecture", "Persian Tokenization"],
                    "requirements": "4GB+ RAM, GPU recommended",
                    "size_gb": 0.6,
                    "quality": "Ø¨Ø§Ù„Ø§"
                },
                {
                    "name": "FastPitch-Persian",
                    "huggingface_id": "persian-tts/fastpitch-persian",
                    "priority": 3,
                    "features": ["Speed Control", "Pitch Control", "Interactive"],
                    "requirements": "GPU + Vocoder",
                    "size_gb": 0.4,
                    "quality": "Ø¨Ø§Ù„Ø§"
                }
            ],
            
            "CPU_OPTIMIZED": [
                {
                    "name": "Silta Persian TTS",
                    "huggingface_id": "persian-tts/silta-persian",
                    "priority": 1,
                    "features": ["Lightweight", "CPU Optimized", "Embedded Systems"],
                    "requirements": "2GB+ RAM, CPU only",
                    "size_gb": 0.2,
                    "quality": "Ù…ØªÙˆØ³Ø·"
                },
                {
                    "name": "ParsiTTS-CPU",
                    "huggingface_id": "persiannlp/parsi-tts-cpu",
                    "priority": 2,
                    "features": ["CPU Optimized", "Persian Optimized"],
                    "requirements": "4GB+ RAM, CPU only",
                    "size_gb": 0.5,
                    "quality": "Ø¨Ø§Ù„Ø§"
                }
            ],
            
            "LOW_END": [
                {
                    "name": "Silta Persian TTS",
                    "huggingface_id": "persian-tts/silta-persian",
                    "priority": 1,
                    "features": ["Lightweight", "CPU Optimized", "Embedded Systems"],
                    "requirements": "2GB+ RAM, CPU only",
                    "size_gb": 0.2,
                    "quality": "Ù…ØªÙˆØ³Ø·"
                },
                {
                    "name": "Basic Persian TTS",
                    "huggingface_id": "persian-tts/basic-tts",
                    "priority": 2,
                    "features": ["Very Lightweight", "Basic Quality"],
                    "requirements": "1GB+ RAM, CPU only",
                    "size_gb": 0.1,
                    "quality": "Ù¾Ø§ÛŒÙ‡"
                }
            ]
        }
        
        self.recommended_models = model_recommendations.get(capability_level, [])
        
        print(f"\nğŸ’¡ Recommended Models for {capability_level}:")
        print("-" * 40)
        
        for i, model in enumerate(self.recommended_models, 1):
            print(f"{i}. {model['name']} ({model['quality']})")
            print(f"   ğŸ“¦ Size: {model['size_gb']}GB")
            print(f"   âš™ï¸ Requirements: {model['requirements']}")
            print(f"   âœ¨ Features: {', '.join(model['features'])}")
            print()
    
    def get_optimal_model(self) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… ÙØ¹Ù„ÛŒ"""
        if self.recommended_models:
            return self.recommended_models[0]  # Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§ÙˆÙ„ÙˆÛŒØª
        return None
    
    def get_all_compatible_models(self) -> List[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±"""
        return self.recommended_models
    
    def can_download_model(self, model_size_gb: float) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù…Ú©Ø§Ù† Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú©"""
        available_space = self.system_info.get('disk_free_gb', 0)
        required_space = model_size_gb * 1.5  # 50% buffer
        
        return available_space >= required_space
    
    def get_system_summary(self) -> Dict:
        """Ø®Ù„Ø§ØµÙ‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³ÛŒØ³ØªÙ…"""
        return {
            'system_info': self.system_info,
            'gpu_info': self.gpu_info,
            'recommended_models': self.recommended_models,
            'optimal_model': self.get_optimal_model()
        }

def main():
    """ØªØ³Øª Ø³ÛŒØ³ØªÙ… ØªØ´Ø®ÛŒØµ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±"""
    detector = HardwareDetector()
    
    print("\n" + "=" * 50)
    print("ğŸ“‹ HARDWARE DETECTION SUMMARY")
    print("Ø®Ù„Ø§ØµÙ‡ ØªØ´Ø®ÛŒØµ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø±")
    print("=" * 50)
    
    optimal_model = detector.get_optimal_model()
    if optimal_model:
        print(f"ğŸ¯ Optimal Model: {optimal_model['name']}")
        print(f"ğŸ“¦ Size: {optimal_model['size_gb']}GB")
        print(f"ğŸ·ï¸ Quality: {optimal_model['quality']}")
        print(f"ğŸ”— Hugging Face: {optimal_model['huggingface_id']}")
        
        if detector.can_download_model(optimal_model['size_gb']):
            print("âœ… Sufficient disk space for download")
        else:
            print("âŒ Insufficient disk space for download")
    else:
        print("âŒ No compatible models found")
    
    return detector

if __name__ == "__main__":
    main()